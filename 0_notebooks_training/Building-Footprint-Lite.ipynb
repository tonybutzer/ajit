{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/aws-samples/aws-open-data-satellite-lidar-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# mkdir -p ~/junk\n",
    "# (cd ~/junk; git clone https://github.com/aws-samples/aws-open-data-satellite-lidar-tutorial.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building-Footprint.ipynb       download-from-s3.sh   Road-Network.ipynb\r\n",
      "Building-Footprint-Lite.ipynb  libs\t\t     Road-Network-Lite.ipynb\r\n",
      "CODE_OF_CONDUCT.md\t       LICENSE\t\t     setup-env.sh\r\n",
      "conda-requirements.txt\t       networks\t\t     THIRD-PARTY\r\n",
      "configs\t\t\t       pip-requirements.txt\r\n",
      "CONTRIBUTING.md\t\t       README.md\r\n"
     ]
    }
   ],
   "source": [
    "! ls /home/ec2-user/junk/aws-open-data-satellite-lidar-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://aws-satellite-lidar-tutorial/data/ ./data/ --recursive --no-sign-request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "# #!/bin/bash\n",
    "\n",
    "# # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# # SPDX-License-Identifier: MIT-0\n",
    "\n",
    "# # This script downloads data used in this tutorial from S3 buckets.\n",
    "\n",
    "# # Exit when error occurs\n",
    "# set -e\n",
    "\n",
    "# echo \"===== Downloading RGB+LiDAR merged data (~22GB) ... =====\"\n",
    "# aws s3 cp s3://aws-satellite-lidar-tutorial/data/ ./data/ --recursive --no-sign-request\n",
    "\n",
    "# echo \"===== Downloading pretrained model weights (617MB) ... =====\"\n",
    "# aws s3 cp s3://aws-satellite-lidar-tutorial/models/ ./models/ --recursive --no-sign-request\n",
    "\n",
    "# echo \"===== Downloading completes. =====\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Building Footprint Extraction Using Satellite RGB and LiDAR Elevation\n",
    "\n",
    "This is the \"lite\" version of building extraction notebook. The purpose of this version is to allow reproduction on free-tier Amazon SageMaker instances. Some network accuracies may be downgraded due to these modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.t3.medium notebook instance. The Amazon SageMaker offers free-tier ml.t3.medium notebook instances, see [webpage](https://aws.amazon.com/sagemaker/pricing/) for more details._\n",
    "\n",
    "First of all, if you haven't done so, please follow instructions in `README.md` to run `setup-env.sh` and `download-from-s3.sh` scripts to properly set up the Conda environment and download necessary files from S3 buckets prepared for this tutorial. Before proceeding, make sure this notebook connects with the proper kernel (`conda_[name]`, `[name]` is name of the new Conda environment you just created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -y -c conda-forge p-tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -y -c conda-forge pytorch\n",
    "#! conda install -y -c conda-forge scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y -c conda-forge torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External modules\n",
    "\n",
    "We made some customized modification to the external module `solaris` in `libs/` directory. Please refer to their [GitHub page](https://github.com/CosmiQ/solaris) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cp -r ~/junk/aws-open-data-satellite-lidar-tutorial/libs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -y -c conda-forge gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/.local/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: setuptools in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorflow) (59.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/.local/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/.local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/.local/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/.local/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /data/miniconda3/envs/lidar1/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/.local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/.local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAIL -- ! conda install -y -c conda-forge tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "     active environment : lidar1\r\n",
      "    active env location : /data/miniconda3/envs/lidar1\r\n",
      "            shell level : 2\r\n",
      "       user config file : /home/ec2-user/.condarc\r\n",
      " populated config files : /home/ec2-user/.condarc\r\n",
      "          conda version : 4.11.0\r\n",
      "    conda-build version : not installed\r\n",
      "         python version : 3.9.7.final.0\r\n",
      "       virtual packages : __linux=4.14.252=0\r\n",
      "                          __glibc=2.26=0\r\n",
      "                          __unix=0=0\r\n",
      "                          __archspec=1=x86_64\r\n",
      "       base environment : /data/miniconda3  (writable)\r\n",
      "      conda av data dir : /data/miniconda3/etc/conda\r\n",
      "  conda av metadata url : None\r\n",
      "           channel URLs : https://conda.anaconda.org/open3d-admin/linux-64\r\n",
      "                          https://conda.anaconda.org/open3d-admin/noarch\r\n",
      "                          https://conda.anaconda.org/conda-forge/linux-64\r\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\r\n",
      "                          https://repo.anaconda.com/pkgs/main/linux-64\r\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\r\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\r\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\r\n",
      "          package cache : /data/miniconda3/pkgs\r\n",
      "                          /home/ec2-user/.conda/pkgs\r\n",
      "       envs directories : /data/miniconda3/envs\r\n",
      "                          /home/ec2-user/.conda/envs\r\n",
      "               platform : linux-64\r\n",
      "             user-agent : conda/4.11.0 requests/2.27.1 CPython/3.9.7 Linux/4.14.252-195.483.amzn2.x86_64 amzn/2 glibc/2.26\r\n",
      "                UID:GID : 1000:1000\r\n",
      "             netrc file : None\r\n",
      "           offline mode : False\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /data/miniconda3/envs/lidar1:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "gdal                      3.4.0           py37h9ed22ba_12    conda-forge\n",
      "libgdal                   3.4.0               h1504ab5_12    conda-forge\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 15:44:41.919403: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-26 15:44:41.919472: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Crop' from 'albumentations.augmentations.transforms' (/data/miniconda3/envs/lidar1/lib/python3.7/site-packages/albumentations/augmentations/transforms.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22058/3773095406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolaris\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.2.2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# define the current directory as `data_dir`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/data/coco.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_df_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_check_geom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_files_recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbbox_corners_to_coco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon_to_coco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_multi_geometries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_logging_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeojson_to_px_gdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_multipolygons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/utils/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/nets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            'weights')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/nets/datagen.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_augs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_aug_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_df_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_geom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ajit/0_notebooks_training/libs/solaris/nets/transform.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDualTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mImageOnlyTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoOp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVerticalFlip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mHorizontalFlip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCutout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRandomSizedCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpticalDistortion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridDistortion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElasticTransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Crop' from 'albumentations.augmentations.transforms' (/data/miniconda3/envs/lidar1/lib/python3.7/site-packages/albumentations/augmentations/transforms.py)"
     ]
    }
   ],
   "source": [
    "import libs.solaris as sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The datasets used in this notebook are saved in directory `./data/buildings/`.\n",
    "\n",
    "1. **SpaceNet satellite images**:\n",
    "There are 3850 tiles (\"chips\") of 200m$\\times$200m satellite images/labels in this training dataset (`SN2/AOI_2_Vegas`). See [link](https://spacenet.ai/las-vegas/) for more information. We extract the ~0.3m resolution pan-sharpened RGB (`PS-RGB`) 3-channel satellite images and perform white balancing. On the other hand, the `geojson_buildings/` directory contains GeoJSON files for building ground truths.\n",
    "\n",
    "2. **USGS 3DEP LiDAR**:\n",
    "This is [a large USGS project](https://www.usgs.gov/core-science-systems/ngp/3dep) that covers many regions throughout US. Also see [here](https://usgs.entwine.io/) for cool visualizations. We extract 3D point clouds in Las Vegas region that overlap with the SpaceNet data, project them into corresponding 2D tiles (3084 in total). The LiDAR data have two attributes: elevation and reflection intensity. For building extraction, we will use the elevation attribute after normalizing the elevation values.\n",
    "\n",
    "At the end, we merge the LiDAR elevation attribute as an additional channel to the RGB image, and save these 4-channel images to `RGB+ELEV/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/buildings/'\n",
    "img_dir = os.path.join(data_dir, 'RGB+ELEV')\n",
    "bldg_dir = os.path.join(data_dir, 'geojson_buildings')\n",
    "\n",
    "# Prefix of all filename - naming convention\n",
    "prefix = 'SN2_buildings_train_AOI_2_Vegas_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a data sample\n",
    "sample = 'img1423' # chip ID, img? format\n",
    "\n",
    "# Read in 4-channel image from GeoTIFF.\n",
    "img_file = prefix + 'RGB+ELEV_' + sample + '.tif'\n",
    "img_path = os.path.join(img_dir, img_file)\n",
    "img = skimage.io.imread(img_path)\n",
    "rgb = img[..., :3]\n",
    "elev = img[..., -1]\n",
    "\n",
    "# Read in GeoJSON file and convert polygons to footprint mask.\n",
    "bldg_file = prefix + 'geojson_buildings_' + sample + '.geojson'\n",
    "bldg_path = os.path.join(bldg_dir, bldg_file)\n",
    "mask = sol.vector.mask.footprint_mask(bldg_path, reference_im=img_path)\n",
    "\n",
    "# Display satellite image and building footprint mask.\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_title('Satellite image')\n",
    "ax[1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[1].set_title('LiDAR elevation')\n",
    "ax[2].imshow(mask, cmap='Blues')\n",
    "ax[2].set_title('Building footprint masks')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ground truths to masks\n",
    "\n",
    "The SpaceNet images come with GeoJSON annotations as polygon vectors. These cannot be directly accepted by neural network; instead, we should convert them to binary mask images (as shown above). These masks will be the \"ground truth labels\" to train our segmentation network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = os.path.join(data_dir, 'mask_buildings')\n",
    "\n",
    "# This step is time-consuming, and only needs to be run once.\n",
    "# After you have all the mask labels ready in folder `mask_buildings/`,\n",
    "# this operation will be skipped.\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.mkdir(mask_dir)\n",
    "    \n",
    "    img_file_list = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "    for img_file in tqdm(img_file_list):\n",
    "        # Get the `img[number]` chip ID from file name.\n",
    "        chip_id = os.path.splitext(img_file)[0].split('_')[-1]\n",
    "        bldg_file = prefix + 'geojson_buildings_' + chip_id + '.geojson'\n",
    "        mask_file = prefix + 'mask_buildings_' + chip_id + '.tif'\n",
    "        \n",
    "        _ = sol.vector.mask.footprint_mask(\n",
    "            os.path.join(bldg_dir, bldg_file),\n",
    "            out_file=os.path.join(mask_dir, mask_file),\n",
    "            reference_im=os.path.join(img_dir, img_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on sample data\n",
    "\n",
    "For this notebook, all training setups in `.yml` format are saved in directory `configs/buildings/`:\n",
    "1. `RGB-only.yml`: network only uses RGB images as input (3-channel);\n",
    "2. `ELEV-only.yml`: network only uses LiDAR elevation images as input (1-channel);\n",
    "3. `RGB+ELEV.yml`: network uses both RGB and LiDAR elevation as merged input (4-channel).\n",
    "\n",
    "In the following cell, please select the config file you would like to experiment. After completing this notebook, feel free to come back here, change to other config or create your own setup to see what's different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------#\n",
    "# Select the config file you want to use\n",
    "config = sol.utils.config.parse('./configs/buildings/RGB+ELEV.yml')\n",
    "# --------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load a customized VGG16-Unet from `networks/` directory. This algorithm is from the winner `XD_XD` from SpaceNet challenge 4 ([link to repo](https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/tree/master/XD_XD)). We modified the code so that it takes multi-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customized multi-channel input VGG16-Unet model.\n",
    "from networks.vgg16_unet import get_modified_vgg16_unet\n",
    "\n",
    "custom_model = get_modified_vgg16_unet(\n",
    "    in_channels=config['data_specs']['channels'])\n",
    "custom_model_dict = {\n",
    "    'model_name': 'modified_vgg16_unet',\n",
    "    'weight_path': config['training']['model_dest_path'],\n",
    "    'weight_url': None,\n",
    "    'arch': custom_model}\n",
    "config['train'] = False\n",
    "# Modification to reduce RAM cost, may impact prediction accuracy.\n",
    "config['batch_size'] = 1\n",
    "config['data_specs']['width'] = 512\n",
    "config['data_specs']['height'] = 512\n",
    "inferer = sol.nets.infer.Inferer(config, custom_model_dict=custom_model_dict)\n",
    "\n",
    "sample_df = pd.DataFrame({'image': [img_path]}) # previous sample image\n",
    "inferer(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction result from the output directory.\n",
    "pred_dir = config['inference']['output_dir']\n",
    "pred_file = prefix + 'RGB+ELEV_' + sample + '.tif'\n",
    "pred_path = os.path.join(pred_dir, pred_file)\n",
    "pred = skimage.io.imread(pred_path)[..., 0]\n",
    "\n",
    "# Display satellite image, LiDAR, prediction mask, and ground truth mask.\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0][0].imshow(rgb)\n",
    "ax[0][0].set_title('Satellite image')\n",
    "ax[0][1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[0][1].set_title('LiDAR elevation')\n",
    "ax[1][0].imshow(pred>0, cmap='Blues')\n",
    "ax[1][0].set_title('Predicted footprint')\n",
    "ax[1][1].imshow(mask, cmap='Blues')\n",
    "ax[1][1].set_title('Ground truth footprint')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks good. Meanwhile, other than visual comparison, a quantitative metric can help us better understand the model's performance. SpaceNet building extraction challenge evaluates models by F-1 scores for IoU>=50%. Now let's evaluate the F-1 score of the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert GeoJSON polygons to binary masks for training and visualization;\n",
    "# similarly, now we will do the reverse: convert prediction masks\n",
    "# to polygon GeoJSON as \"proposal\".\n",
    "results_dir = Path(config['inference']['output_dir']).parent\n",
    "prop_dir = os.path.join(results_dir, 'prop_geojson')\n",
    "os.makedirs(prop_dir, exist_ok=True)\n",
    "prop_file = bldg_file # same filename as the ground truth GeoJSON\n",
    "prop_path = os.path.join(prop_dir, prop_file)\n",
    "prop = sol.vector.mask.mask_to_poly_geojson(\n",
    "    pred_arr=pred,\n",
    "    reference_im=img_path,\n",
    "    do_transform=True,\n",
    "    min_area=1e-10,\n",
    "    output_path=prop_path\n",
    ") # save to geojson file in the output folder\n",
    "\n",
    "# Create evaluator and load the ground truth.\n",
    "evaluator = sol.eval.base.Evaluator(bldg_path)\n",
    "\n",
    "# Evaluate F-1 score by IoU>=50% detections.\n",
    "evaluator.load_proposal(prop_path, conf_field_list=[]) # only one class\n",
    "score = evaluator.eval_iou(miniou=0.5, calculate_class_scores=False)\n",
    "score_df = pd.DataFrame.from_records(score)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer on Random Image\n",
    "\n",
    "In this section, we visually check some results. The next cell randomly sample from the result list. Rerun as many times as you wish to see different figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available images.\n",
    "img_file_list = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "img_file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly check the network predictions. Feel free to rerun this cell!\n",
    "file = random.choice(img_file_list)\n",
    "# Or you can select the 'img1423.tif' we saw before.\n",
    "# file = 'SN2_buildings_train_AOI_2_Vegas_RGB+ELEV_img1423.tif'\n",
    "img_path = os.path.join(img_dir, file)\n",
    "pred_path = os.path.join(pred_dir, file)\n",
    "mask_path = os.path.join(mask_dir, file.replace('RGB+ELEV', 'mask_buildings'))\n",
    "\n",
    "# Perform one-time inference on the selected image.\n",
    "infer_df = pd.DataFrame({'image': [img_path]})\n",
    "inferer(infer_df)\n",
    "\n",
    "# Visualize the result.\n",
    "img = skimage.io.imread(img_path)\n",
    "rgb = img[..., :3]\n",
    "elev = img[..., -1]\n",
    "pred = skimage.io.imread(pred_path)[..., 0]\n",
    "mask = skimage.io.imread(mask_path)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0][0].imshow(rgb)\n",
    "ax[0][0].set_title('RGB - {}'.format(file.split('_')[-1]))\n",
    "ax[0][1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[0][1].set_title('LIDAR Elevation')\n",
    "ax[1][0].imshow(pred>0, cmap='Blues')\n",
    "ax[1][0].set_title('Prediction')\n",
    "ax[1][1].imshow(mask, cmap='Blues')\n",
    "ax[1][1].set_title('Ground Truth')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
